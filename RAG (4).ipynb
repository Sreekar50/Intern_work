{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Saogl7UFyWRI",
        "outputId": "fd01fda8-dcff-4b4d-8445-5c77b9bab592"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers\n",
        "!pip install -q langchain\n",
        "!pip install -q rank-bm25\n",
        "!pip install -q faiss-cpu  # Use faiss-gpu if you have GPU and need faster performance\n",
        "!pip install -q numpy\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "azlAfS2EzITP",
        "outputId": "f378e0f3-fbb2-4142-a22a-a1583a31e632"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start Ollama service in the background\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Start Ollama server in background\n",
        "subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(5)  # Wait for server to start\n",
        "\n",
        "print(\"✅ Ollama installed and server started!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zi3eZu7WzKmN",
        "outputId": "d731d7cf-a239-4f1a-badd-ff2d5429a81a"
      },
      "outputs": [],
      "source": [
        "print(\"Pulling llama2:7b model...\")\n",
        "!ollama pull llama2:7b\n",
        "\n",
        "print(\"\\nPulling llama3:latest model...\")\n",
        "!ollama pull llama3:latest\n",
        "\n",
        "print(\"\\n✅ Models downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "By-Bscr_zYYP",
        "outputId": "3512b35e-0621-48aa-f6b5-2e5e32ccf5a0"
      },
      "outputs": [],
      "source": [
        "print(\"Verifying installations...\\n\")\n",
        "\n",
        "# Check sentence-transformers\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    print(\"✅ sentence-transformers: OK\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ sentence-transformers: {e}\")\n",
        "\n",
        "# Check langchain\n",
        "try:\n",
        "    from langchain.schema import Document\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    print(\"✅ langchain: OK\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ langchain: {e}\")\n",
        "\n",
        "# Check rank-bm25\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    print(\"✅ rank-bm25: OK\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ rank-bm25: {e}\")\n",
        "\n",
        "# Check faiss\n",
        "try:\n",
        "    import faiss\n",
        "    print(\"✅ faiss: OK\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ faiss: {e}\")\n",
        "\n",
        "# Check ollama\n",
        "try:\n",
        "    import ollama\n",
        "    # Test connection\n",
        "    response = ollama.chat(\n",
        "        model='llama2:7b',\n",
        "        messages=[{'role': 'user', 'content': 'Say \"Hello from Ollama!\"'}]\n",
        "    )\n",
        "    print(f\"✅ ollama: OK - {response['message']['content']}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ollama: {e}\")\n",
        "\n",
        "print(\"\\n✅ All installations verified!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "212yNvb0qGyY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "import ollama\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from rank_bm25 import BM25Okapi\n",
        "import faiss\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# ==========================\n",
        "# Data Loading & Preprocessing\n",
        "# ==========================\n",
        "def load_and_preprocess_recipes(file_path: str) -> List[Dict]:\n",
        "    \"\"\"Load recipes and clean the data\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        recipes = json.load(f)\n",
        "\n",
        "    cleaned_recipes = []\n",
        "    for recipe in recipes:\n",
        "        # Check if essential fields exist\n",
        "        if all(recipe.get(field) not in [None, \"\", \"NaN\"] and\n",
        "               not (isinstance(recipe.get(field), float) and math.isnan(recipe.get(field)))\n",
        "               for field in ['title', 'recipe_url', 'category']):\n",
        "\n",
        "            # Parse and clean numeric fields\n",
        "            total_time = int(re.search(r'\\d+', recipe['total_time']).group()) \\\n",
        "                        if isinstance(recipe['total_time'], str) and re.search(r'\\d+', recipe['total_time']) else 0\n",
        "            servings = int(float(recipe['servings'])) \\\n",
        "                      if str(recipe['servings']).replace('.', '', 1).isdigit() else 0\n",
        "            calories = float(recipe['calories']) \\\n",
        "                      if str(recipe['calories']).replace('.', '', 1).isdigit() else 0.0\n",
        "            protein = float(recipe['protein']) \\\n",
        "                     if str(recipe['protein']).replace('.', '', 1).isdigit() else 0.0\n",
        "            fat = float(recipe['total_fat']) \\\n",
        "                 if str(recipe['total_fat']).replace('.', '', 1).isdigit() else 0.0\n",
        "\n",
        "            ingredients = recipe['ingredients'] if recipe['ingredients'] not in [None, \"\", \"NaN\"] else \"Not Available\"\n",
        "            directions = recipe['directions'] if recipe['directions'] not in [None, \"\", \"NaN\"] else \"Not Available\"\n",
        "\n",
        "            cleaned_recipe = {\n",
        "                'title': recipe['title'],\n",
        "                'category': recipe['category'],\n",
        "                'url': recipe['recipe_url'],\n",
        "                'total_time': total_time,\n",
        "                'servings': servings,\n",
        "                'ingredients': ingredients,\n",
        "                'directions': directions,\n",
        "                'calories': calories,\n",
        "                'protein': protein,\n",
        "                'fat': fat\n",
        "            }\n",
        "            cleaned_recipes.append(cleaned_recipe)\n",
        "\n",
        "    return cleaned_recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RHUcCEVxqSGX"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# Chunking Strategy\n",
        "# ==========================\n",
        "def chunk_recipes(recipes: List[Dict], chunk_size: int = 500, chunk_overlap: int = 100) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Chunk recipes intelligently - keep ingredients and directions together when possible\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "\n",
        "    for idx, recipe in enumerate(recipes):\n",
        "        # Ensure directions is a string\n",
        "        directions = str(recipe['directions']) if recipe['directions'] and recipe['directions'] != 'Not Available' else \"\"\n",
        "        ingredients = str(recipe['ingredients']) if recipe['ingredients'] and recipe['ingredients'] != 'Not Available' else \"\"\n",
        "\n",
        "        # Create main recipe text\n",
        "        main_text = f\"\"\"Title: {recipe['title']}\n",
        "Category: {recipe['category']}\n",
        "Total Time: {recipe['total_time']} mins\n",
        "Servings: {recipe['servings']}\n",
        "Calories: {recipe['calories']}\n",
        "Protein: {recipe['protein']}g\n",
        "Fat: {recipe['fat']}g\"\"\"\n",
        "\n",
        "        # Create ingredient chunk\n",
        "        ingredients_text = f\"\"\"Recipe: {recipe['title']}\n",
        "Ingredients:\n",
        "{ingredients}\"\"\"\n",
        "\n",
        "        # Create directions chunk\n",
        "        directions_text = f\"\"\"Recipe: {recipe['title']}\n",
        "Cooking Instructions:\n",
        "{directions}\"\"\"\n",
        "\n",
        "        # Add metadata to each chunk\n",
        "        metadata = {\n",
        "            'recipe_id': idx,\n",
        "            'title': recipe['title'],\n",
        "            'url': recipe['url'],\n",
        "            'category': recipe['category'],\n",
        "            'total_time': recipe['total_time'],\n",
        "            'servings': recipe['servings'],\n",
        "            'calories': recipe['calories'],\n",
        "            'protein': recipe['protein'],\n",
        "            'fat': recipe['fat']\n",
        "        }\n",
        "\n",
        "        # Create documents for each logical chunk\n",
        "        documents.append(Document(page_content=main_text, metadata={**metadata, 'chunk_type': 'summary'}))\n",
        "        documents.append(Document(page_content=ingredients_text, metadata={**metadata, 'chunk_type': 'ingredients'}))\n",
        "        documents.append(Document(page_content=directions_text, metadata={**metadata, 'chunk_type': 'directions'}))\n",
        "\n",
        "        # For very long recipes, use additional splitting\n",
        "        if directions and len(directions) > chunk_size:\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=chunk_size,\n",
        "                chunk_overlap=chunk_overlap,\n",
        "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "            )\n",
        "            direction_chunks = text_splitter.split_text(directions)\n",
        "            for i, chunk in enumerate(direction_chunks):\n",
        "                chunk_text = f\"Recipe: {recipe['title']}\\nCooking Instructions (Part {i+1}):\\n{chunk}\"\n",
        "                documents.append(Document(\n",
        "                    page_content=chunk_text,\n",
        "                    metadata={**metadata, 'chunk_type': f'directions_part_{i+1}'}\n",
        "                ))\n",
        "\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HiaIo9xnqYlv"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# FAISS Vector Store Setup\n",
        "# ==========================\n",
        "class RecipeVectorStore:\n",
        "    def __init__(self, embedding_model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
        "        self.dimension = self.embedding_model.get_sentence_embedding_dimension()\n",
        "        self.index = None\n",
        "        self.documents = []\n",
        "        self.embeddings = None\n",
        "\n",
        "    def build_index(self, documents: List[Document]):\n",
        "        \"\"\"Build FAISS index from documents\"\"\"\n",
        "        self.documents = documents\n",
        "\n",
        "        # Generate embeddings\n",
        "        texts = [doc.page_content for doc in documents]\n",
        "        print(f\"Generating embeddings for {len(texts)} document chunks...\")\n",
        "        self.embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        # Create FAISS index\n",
        "        # Using IndexFlatIP for inner product (cosine similarity with normalized vectors)\n",
        "        self.index = faiss.IndexFlatIP(self.dimension)\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "\n",
        "        # Add to index\n",
        "        self.index.add(self.embeddings.astype('float32'))\n",
        "        print(f\"FAISS index built with {self.index.ntotal} vectors\")\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"Search using FAISS\"\"\"\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        distances, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
        "\n",
        "        results = []\n",
        "        for idx, distance in zip(indices[0], distances[0]):\n",
        "            if idx < len(self.documents):\n",
        "                results.append((self.documents[idx], float(distance)))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_index(self, path: str):\n",
        "        \"\"\"Save FAISS index to disk\"\"\"\n",
        "        faiss.write_index(self.index, f\"{path}.index\")\n",
        "        with open(f\"{path}_docs.json\", 'w') as f:\n",
        "            json.dump([{\n",
        "                'content': doc.page_content,\n",
        "                'metadata': doc.metadata\n",
        "            } for doc in self.documents], f)\n",
        "        np.save(f\"{path}_embeddings.npy\", self.embeddings)\n",
        "\n",
        "    def load_index(self, path: str):\n",
        "        \"\"\"Load FAISS index from disk\"\"\"\n",
        "        self.index = faiss.read_index(f\"{path}.index\")\n",
        "        with open(f\"{path}_docs.json\", 'r') as f:\n",
        "            docs_data = json.load(f)\n",
        "            self.documents = [Document(page_content=d['content'], metadata=d['metadata'])\n",
        "                            for d in docs_data]\n",
        "        self.embeddings = np.load(f\"{path}_embeddings.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y7SZlyRrqe7i"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# BM25 Setup\n",
        "# ==========================\n",
        "def build_bm25_index(documents: List[Document]) -> BM25Okapi:\n",
        "    \"\"\"Build BM25 index for keyword-based retrieval\"\"\"\n",
        "    tokenized_corpus = [doc.page_content.lower().split() for doc in documents]\n",
        "    return BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# ==========================\n",
        "# Hybrid Retrieval\n",
        "# ==========================\n",
        "class HybridRetriever:\n",
        "    def __init__(self, vector_store: RecipeVectorStore, bm25_index: BM25Okapi,\n",
        "                 documents: List[Document], alpha: float = 0.5):\n",
        "        \"\"\"\n",
        "        alpha: weight for dense retrieval (1-alpha for BM25)\n",
        "        \"\"\"\n",
        "        self.vector_store = vector_store\n",
        "        self.bm25_index = bm25_index\n",
        "        self.documents = documents\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5, filter_time: int = None) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Hybrid retrieval combining dense (FAISS) and sparse (BM25) search\n",
        "        \"\"\"\n",
        "        # Dense retrieval\n",
        "        dense_results = self.vector_store.search(query, top_k=top_k * 3)\n",
        "\n",
        "        # BM25 retrieval\n",
        "        query_tokens = query.lower().split()\n",
        "        bm25_scores = self.bm25_index.get_scores(query_tokens)\n",
        "        bm25_ranked_indices = np.argsort(bm25_scores)[::-1][:top_k * 3]\n",
        "\n",
        "        # Combine scores using Reciprocal Rank Fusion\n",
        "        doc_scores = {}\n",
        "\n",
        "        # Add dense scores\n",
        "        for rank, (doc, score) in enumerate(dense_results):\n",
        "            doc_id = id(doc)\n",
        "            if doc_id not in doc_scores:\n",
        "                doc_scores[doc_id] = {'doc': doc, 'score': 0}\n",
        "            doc_scores[doc_id]['score'] += self.alpha * (1 / (rank + 1))\n",
        "\n",
        "        # Add BM25 scores\n",
        "        for rank, idx in enumerate(bm25_ranked_indices):\n",
        "            doc = self.documents[idx]\n",
        "            doc_id = id(doc)\n",
        "            if doc_id not in doc_scores:\n",
        "                doc_scores[doc_id] = {'doc': doc, 'score': 0}\n",
        "            doc_scores[doc_id]['score'] += (1 - self.alpha) * (1 / (rank + 1))\n",
        "\n",
        "        # Filter by cooking time if specified\n",
        "        if filter_time:\n",
        "            doc_scores = {\n",
        "                k: v for k, v in doc_scores.items()\n",
        "                if abs(v['doc'].metadata.get('total_time', 0) - filter_time) <= filter_time * 0.3\n",
        "            }\n",
        "\n",
        "        # Sort by combined score\n",
        "        ranked_docs = sorted(doc_scores.values(), key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "        # Remove duplicates based on recipe title and return top_k\n",
        "        seen_titles = set()\n",
        "        unique_docs = []\n",
        "        for item in ranked_docs:\n",
        "            title = item['doc'].metadata.get('title')\n",
        "            if title not in seen_titles:\n",
        "                seen_titles.add(title)\n",
        "                unique_docs.append(item['doc'])\n",
        "                if len(unique_docs) >= top_k:\n",
        "                    break\n",
        "\n",
        "        return unique_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-G8_Wt-lqkQR"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# Recipe Generation with RAG\n",
        "# ==========================\n",
        "def generate_recipe_with_rag(cooking_time: int, retrieved_docs: List[Document],\n",
        "                             model: str = 'llama2:7b') -> str:\n",
        "    \"\"\"Generate recipe using retrieved context\"\"\"\n",
        "\n",
        "    # Organize retrieved context\n",
        "    context_parts = []\n",
        "    for doc in retrieved_docs:\n",
        "        context_parts.append(f\"--- Example Recipe ---\\n{doc.page_content}\\n\")\n",
        "\n",
        "    retrieved_context = \"\\n\".join(context_parts)\n",
        "\n",
        "    prompt = f\"\"\"You are an expert cook. Using the following recipe examples as guidance, generate a new unique recipe that meets the given cooking time constraint.\n",
        "\n",
        "IMPORTANT CONSTRAINTS:\n",
        "- Total cooking time must be approximately {cooking_time} minutes (±10%)\n",
        "- Cooking includes: boiling, baking, heating, frying, sautéing, grilling, roasting, or steaming\n",
        "- Do NOT include preparation, refrigeration, or cooling time in the cooking time\n",
        "\n",
        "OUTPUT FORMAT (follow exactly):\n",
        "Recipe Title: [Clear & concise name]\n",
        "\n",
        "Ingredients:\n",
        "[List ingredients with exact quantities, one per line]\n",
        "\n",
        "Instructions:\n",
        "[Numbered step-by-step directions]\n",
        "\n",
        "The new recipe must be:\n",
        "1. Different from the examples but inspired by their style\n",
        "2. Realistic and achievable within the cooking time\n",
        "3. Include proper measurements and clear instructions\n",
        "\n",
        "---\n",
        "EXAMPLE RECIPES FOR INSPIRATION:\n",
        "{retrieved_context}\n",
        "---\n",
        "\n",
        "Generate the new recipe now:\"\"\"\n",
        "\n",
        "    response = ollama.chat(\n",
        "        model=model,\n",
        "        messages=[{'role': 'user', 'content': prompt}]\n",
        "    )\n",
        "    return response['message']['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-CKHovrHqrl_"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# Evaluation\n",
        "# ==========================\n",
        "def evaluate_recipe(recipe_text: str, expected_time: int, model: str = 'llama3:latest') -> Dict:\n",
        "    \"\"\"Evaluate generated recipe using LLM\"\"\"\n",
        "\n",
        "    evaluation_prompt = f\"\"\"You are a strict recipe evaluation expert. Evaluate the following generated recipe against these metrics:\n",
        "\n",
        "1. Faithfulness (Is the information factually correct and realistic?)\n",
        "2. Time Adherence (Does it match the {expected_time} minute cooking time constraint?)\n",
        "3. Coherence (Is it easy to follow and logically ordered?)\n",
        "4. Completeness (Are ingredients, quantities, and steps complete?)\n",
        "5. Practicality (Can this recipe actually be executed as written?)\n",
        "\n",
        "Recipe to evaluate:\n",
        "---\n",
        "{recipe_text}\n",
        "---\n",
        "\n",
        "Provide a score for each metric out of 5, and brief 1-2 line feedback.\n",
        "\n",
        "Format your response EXACTLY as:\n",
        "Faithfulness: X/5 - [feedback]\n",
        "Time Adherence: X/5 - [feedback]\n",
        "Coherence: X/5 - [feedback]\n",
        "Completeness: X/5 - [feedback]\n",
        "Practicality: X/5 - [feedback]\"\"\"\n",
        "\n",
        "    response = ollama.chat(\n",
        "        model=model,\n",
        "        messages=[{'role': 'user', 'content': evaluation_prompt}]\n",
        "    )\n",
        "\n",
        "    return response['message']['content']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492,
          "referenced_widgets": [
            "03d14063b3d24278abb4cfbd4d3eac93",
            "b700965ccda34c71bb7bfed44552307c",
            "66b01adc0b274e6a99416d602a08bf36",
            "d643df3386e546799089deea3ecec136",
            "05dae1874d154a32bcc3aa09f3ef3c41",
            "47230f99917f4ee9b97aec1249e5cf7e",
            "b78b3058d42e44aab20152d3b504b03d",
            "3455ee74bddd46b9afd07fdb265db19f",
            "636f09bfebd3462e85c687e81b593592",
            "3257cf4b1f7344219efcbd45818557ca",
            "dbe88174bc304fd4a84f4adde81280be"
          ]
        },
        "collapsed": true,
        "id": "Kr2bRMqIq-Nk",
        "outputId": "29ec231f-e019-47e8-dc07-84eea2145ef6"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# Main Pipeline\n",
        "# ==========================\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading recipes...\")\n",
        "    recipes = load_and_preprocess_recipes('/content/recipes.json')\n",
        "    print(f\"Loaded {len(recipes)} recipes\")\n",
        "\n",
        "    # Chunk recipes\n",
        "    print(\"\\nChunking recipes...\")\n",
        "    documents = chunk_recipes(recipes)\n",
        "    print(f\"Created {len(documents)} document chunks\")\n",
        "\n",
        "    # Build vector store\n",
        "    print(\"\\nBuilding FAISS vector store...\")\n",
        "    vector_store = RecipeVectorStore()\n",
        "    vector_store.build_index(documents)\n",
        "\n",
        "    # Build BM25 index\n",
        "    print(\"\\nBuilding BM25 index...\")\n",
        "    bm25_index = build_bm25_index(documents)\n",
        "\n",
        "    # Create hybrid retriever\n",
        "    print(\"\\nInitializing hybrid retriever...\")\n",
        "    retriever = HybridRetriever(vector_store, bm25_index, documents, alpha=0.6)\n",
        "\n",
        "    # Generate recipes for different cooking times\n",
        "    cooking_times = [2, 5, 10, 40, 90, 150]\n",
        "    num_recipes_per_time = 5\n",
        "    all_results = {}\n",
        "\n",
        "    for cooking_time in cooking_times:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Generating recipes for {cooking_time} minutes cooking time\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        all_results[cooking_time] = []\n",
        "\n",
        "        for i in range(num_recipes_per_time):\n",
        "            print(f\"\\nGenerating recipe {i+1}/{num_recipes_per_time}...\")\n",
        "\n",
        "            # Retrieve relevant recipes\n",
        "            query = f\"recipes that can be cooked in {cooking_time} minutes\"\n",
        "            retrieved_docs = retriever.retrieve(query, top_k=3, filter_time=cooking_time)\n",
        "\n",
        "            print(f\"Retrieved {len(retrieved_docs)} relevant recipe chunks\")\n",
        "\n",
        "            # Generate recipe\n",
        "            generated_recipe = generate_recipe_with_rag(cooking_time, retrieved_docs)\n",
        "\n",
        "            # Evaluate\n",
        "            evaluation = evaluate_recipe(generated_recipe, cooking_time)\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'recipe': generated_recipe,\n",
        "                'evaluation': evaluation,\n",
        "                'retrieved_sources': [doc.metadata['title'] for doc in retrieved_docs]\n",
        "            }\n",
        "            all_results[cooking_time].append(result)\n",
        "\n",
        "            print(f\"\\nGenerated Recipe:\\n{generated_recipe}\")\n",
        "            print(f\"\\nEvaluation:\\n{evaluation}\")\n",
        "\n",
        "    # Save results\n",
        "    print(\"\\nSaving results...\")\n",
        "    with open('rag_generated_recipes.json', 'w') as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "\n",
        "    # Save index for future use\n",
        "    print(\"Saving FAISS index...\")\n",
        "    vector_store.save_index('recipe_index')\n",
        "\n",
        "    print(\"\\n✅ Pipeline completed successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03d14063b3d24278abb4cfbd4d3eac93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b700965ccda34c71bb7bfed44552307c",
              "IPY_MODEL_66b01adc0b274e6a99416d602a08bf36",
              "IPY_MODEL_d643df3386e546799089deea3ecec136"
            ],
            "layout": "IPY_MODEL_05dae1874d154a32bcc3aa09f3ef3c41"
          }
        },
        "05dae1874d154a32bcc3aa09f3ef3c41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3257cf4b1f7344219efcbd45818557ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3455ee74bddd46b9afd07fdb265db19f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47230f99917f4ee9b97aec1249e5cf7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "636f09bfebd3462e85c687e81b593592": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66b01adc0b274e6a99416d602a08bf36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3455ee74bddd46b9afd07fdb265db19f",
            "max": 953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_636f09bfebd3462e85c687e81b593592",
            "value": 8
          }
        },
        "b700965ccda34c71bb7bfed44552307c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47230f99917f4ee9b97aec1249e5cf7e",
            "placeholder": "​",
            "style": "IPY_MODEL_b78b3058d42e44aab20152d3b504b03d",
            "value": "Batches:   1%"
          }
        },
        "b78b3058d42e44aab20152d3b504b03d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d643df3386e546799089deea3ecec136": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3257cf4b1f7344219efcbd45818557ca",
            "placeholder": "​",
            "style": "IPY_MODEL_dbe88174bc304fd4a84f4adde81280be",
            "value": " 8/953 [00:36&lt;1:04:00,  4.06s/it]"
          }
        },
        "dbe88174bc304fd4a84f4adde81280be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
