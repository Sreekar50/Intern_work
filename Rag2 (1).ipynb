{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 17 09:18:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   61C    P0            165W /  250W |   18385MiB /  40960MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCIE-40GB          On  |   00000000:81:00.0 Off |                    0 |\n",
      "| N/A   60C    P0            152W /  250W |   15974MiB /  40960MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   2446810      C   python                                       9926MiB |\n",
      "|    0   N/A  N/A   2453581      C   python                                       3460MiB |\n",
      "|    0   N/A  N/A   2467700      C   ...611/anaconda3/envs/kanak/bin/python        890MiB |\n",
      "|    0   N/A  N/A   2533526      C   ...eepak21319/deepak/.conda/bin/python        606MiB |\n",
      "|    0   N/A  N/A   3652626      C   python                                       3472MiB |\n",
      "|    1   N/A  N/A   2447947      C   python3                                     14550MiB |\n",
      "|    1   N/A  N/A   2467700      C   ...611/anaconda3/envs/kanak/bin/python        630MiB |\n",
      "|    1   N/A  N/A   3652626      C   python                                        772MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2540595/3445707328.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  \n",
    "    model_kwargs={\"device\": device},  \n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "recipes_file = \"final_recipes_lakhs.json\"\n",
    "jq_schema = \".[] | {title, ingredients, directions, total_time}\"\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=recipes_file,\n",
    "    jq_schema=jq_schema,\n",
    "    text_content=False,  \n",
    "    metadata_func=lambda record, metadata: {\n",
    "        \"title\": record.get(\"title\"),\n",
    "        \"total_time\": record.get(\"total_time\")\n",
    "    },\n",
    "    json_lines=False\n",
    ")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in data:\n",
    "    content = json.loads(doc.page_content)\n",
    "    doc.page_content = f\"\"\"\n",
    "    Recipe Title: {content['title']}\n",
    "    Ingredients: {content['ingredients']}\n",
    "    Directions: {content['directions']}\n",
    "    Total Time: {content['total_time']}\n",
    "    \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Recipe Title: Linda's Special Potato Salad\n",
      "    Ingredients: 5 lbs    red potatoes, with skins on ; 2 cups    mayonnaise (more or less depending on how you like it, Do NOT use fat free or low fat) ; 1 large    onion, finely diced ; 2 stalks    celery, finely diced  or 1   teaspoon    celery seed ; 3 carrots, finely diced ; 1 green pepper, finely diced ; 6 hard-boiled eggs, finely chopped ; 3 tablespoons   fresh finely chopped chives ; 20 green olives, finely chopped ; 1⁄4 cup    yellow mustard ; 2 tablespoons    cider vinegar ; 1 tablespoon    sugar ; 1⁄2 teaspoon    salt ; 1 teaspoon    black pepper\n",
      "    Directions: Peel potatoes, and boil until just done, about 20 minutes. (Just until your fork can be inserted all the way through the potato. You don't want them mushy). Drain well and let cool, then dice or slice potatoes. (I like a small dice, about 1/2\"x1/2\"). The potatoes will cut nicely once chilled and won't fall apart. Mix other ingredients, and add to chilled potatoes. Let set in the fridge for at least 2 hours before serving for flavors to meld. Serve chilled.\n",
      "    Total Time: 70 mins' metadata={'title': \"Linda's Special Potato Salad\", 'total_time': '70 mins'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split recipes into 1191026 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Reduced for recipe context\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \"],  # Better semantic breaks\n",
    "    add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "print(f\"Split recipes into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# vector_store = FAISS.from_documents(\n",
    "#     documents=all_splits,\n",
    "#     embedding=embeddings\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "# Save the vector_store to local storage\n",
    "# vector_store.save_local(\"vector_store_data\")\n",
    "\n",
    "# Load the vector_store from local storage\n",
    "loaded_vector_store = FAISS.load_local(\"vector_store_data\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(all_splits)\n",
    "bm25_retriever.k = 3\n",
    "\n",
    "# ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        loaded_vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        bm25_retriever\n",
    "    ],\n",
    "    weights=[0.7, 0.3]\n",
    ")\n",
    "\n",
    "def retrieve_recipe(query, top_k=3):\n",
    "    return ensemble_retriever.get_relevant_documents(query)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2540595/1805776034.py:17: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return ensemble_retriever.get_relevant_documents(query)[:top_k]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='45ba67d6-884a-4cd1-8939-62515d70bc81', metadata={'title': 'Baked Rib Eye Steaks With Mushrooms', 'total_time': '26 mins', 'start_index': 1084}, page_content='. I can have supper on the table in 30 minutes. This recipe gives me time to make a salad and potatoes or pasta.'), Document(id='8f0f5d18-82dd-4a25-b218-72ffce3353b3', metadata={'title': 'Aussie Bacon and Egg Pie', 'total_time': '65 mins', 'start_index': 1101}, page_content='. Reduce the temperature to moderate and then cook for another 25 to 30 minutes. Serves hot or cold.'), Document(id='20b70f32-bc87-40e3-b75a-0f4e8cf1f50a', metadata={'title': 'Turkey Chili Soup', 'total_time': '360 mins', 'start_index': 1629}, page_content='. Add onions and garlic, sauteing until onions are translucent. Add celery, carrot, and potatoes. Sweat over medium heat for 30 minutes, until the vegetables are soft, stirring occasionally. Add chili powder, chipotle, cumin, paprika, and cinnamon, cooking over low heat for 10 minutes and stirring occasionally. Add tomatoes, 6 cups stock, beans, peppers, wine and cilantro, simmering for 35 to 40 minutes. Add turkey, mustard greens, corn and vinegar. Cook for 15 minutes')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the recipes which can be cooked in exactly 30 minutes?\"\n",
    "retrieved_docs = retrieve_recipe(query, top_k=3)\n",
    "print(retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# # 1. Load Llama-2 7B\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# # 2. Create Prompt Function\n",
    "# def create_prompt(cooking_time):\n",
    "#     return f\"\"\"<|user|>\n",
    "# You are an expert cook. Generate a recipe that meets the following criteria:\n",
    "\n",
    "# Cooking Time: The total cooking time must be ≤ {cooking_time} minutes. \n",
    "# Cooking includes boiling, baking, heating, frying, sautéing, grilling, roasting, or steaming.\n",
    "# It does not include preparation, refrigeration, or cooling time.\n",
    "\n",
    "# Format: The generated recipe must strictly follow this structure:\n",
    "\n",
    "# Recipe Title: [Clear & concise name]\n",
    "# Ingredients: [List ingredients with exact quantities]\n",
    "# Instructions: [Step-by-step directions]\n",
    "\n",
    "# The recipe must not exceed {cooking_time} minutes under any circumstances. Be precise about the time.\n",
    "# </s>\n",
    "# <|assistant|>\n",
    "# \"\"\"\n",
    "\n",
    "# # 3. Response Generation Function\n",
    "# def generate_llama_response(cooking_time, retrieved_docs):\n",
    "#     # Format context from retrieved documents\n",
    "#     context = \"\\n\".join([d.page_content for d in retrieved_docs])\n",
    "    \n",
    "#     # Create the prompt dynamically based on cooking time\n",
    "#     prompt = create_prompt(cooking_time)\n",
    "    \n",
    "#     # Combine context and prompt\n",
    "#     full_prompt = f\"{context}\\n\\n{prompt}\"\n",
    "    \n",
    "#     # Tokenize and generate response\n",
    "#     inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(\n",
    "#         inputs.input_ids,\n",
    "#         max_new_tokens=512,\n",
    "#         temperature=0.7,\n",
    "#         top_p=0.9\n",
    "#     )\n",
    "    \n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # 4. Full Pipeline Execution\n",
    "# query = \"What are the recipes which can be cooked in exactly 120 minutes?\"\n",
    "# retrieved_docs = retrieve_recipe(query, top_k=3)  # Assuming retrieve_recipe is defined elsewhere\n",
    "# response = generate_llama_response(cooking_time=120, retrieved_docs=retrieved_docs)\n",
    "\n",
    "# print(\"Final Answer:\", response.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pickle\n",
    "import random\n",
    "from rapidfuzz import fuzz\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Ensure Ollama has LLaMA 2 7B installed\n",
    "OLLAMA_MODEL = \"llama2\"\n",
    "\n",
    "# Unique categories tracking\n",
    "used_proteins = set()\n",
    "used_cuisines = set()\n",
    "used_methods = set()\n",
    "\n",
    "# Available categories\n",
    "all_proteins = {\"chicken\", \"beef\", \"tofu\", \"lentils\", \"pork\", \"salmon\", \"mushrooms\", \"duck\"}\n",
    "all_cuisines = {\"Italian\", \"Thai\", \"Korean\", \"Indian\", \"Mexican\", \"Moroccan\", \"French\", \"Chinese\"}\n",
    "all_methods = {\"grilling\", \"stir-frying\", \"baking\", \"boiling\", \"steaming\", \"roasting\"}\n",
    "\n",
    "# Function to ensure variety\n",
    "def get_unique_category(existing, all_options):\n",
    "    available_options = list(all_options - existing)\n",
    "    if not available_options:  # Reset if all are used\n",
    "        existing.clear()\n",
    "        available_options = list(all_options)\n",
    "    choice = random.choice(available_options)\n",
    "    existing.add(choice)\n",
    "    return choice\n",
    "\n",
    "# Generate recipe prompt with enforced uniqueness\n",
    "def create_prompt(cooking_time):\n",
    "    # unique_protein = get_unique_category(used_proteins, all_proteins)\n",
    "    # unique_cuisine = get_unique_category(used_cuisines, all_cuisines)\n",
    "    # unique_method = get_unique_category(used_methods, all_methods)\n",
    "\n",
    "    return f\"\"\"<|user|>\n",
    "                    You are an expert cook. Generate a recipe that meets the following criteria:\n",
    "\n",
    "\n",
    "Cooking Time: The total cooking time must be <= {cooking_time} minutes. \n",
    "Cooking includes boiling, baking, heating, frying, sautÃ©ing, grilling, roasting, or steaming.\n",
    "It does not include preparation, refrigeration, or cooling time.                    \n",
    "Format: The generated recipe must strictly follow this structure:\n",
    "\n",
    "                    Recipe Title: [Clear & concise name]\n",
    "                    Ingredients: [List ingredients with exact quantities]\n",
    "                    Instructions: [Step-by-step directions]\n",
    "\n",
    "\n",
    "                        The recipe must not exceed {cooking_time} minutes under any circumstances. Be precise about the time.\n",
    "                        </s>\n",
    "                        <|assistant|>\n",
    "                        \"\"\"\n",
    "\n",
    "# Retrieve diverse recipe samples\n",
    "def retrieve_recipe(query, top_k=5):\n",
    "    retrieved_docs = ensemble_retriever.get_relevant_documents(query)\n",
    "    return random.sample(retrieved_docs, min(len(retrieved_docs), top_k))  # Random selection for variety\n",
    "\n",
    "# Levenshtein similarity to prevent near-duplicates\n",
    "def is_unique(recipe, existing_recipes, threshold=80):\n",
    "    for r in existing_recipes:\n",
    "        if fuzz.ratio(recipe, r) > threshold:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Generate a recipe with enforced diversity\n",
    "def generate_ollama_response(cooking_time, retrieved_docs):\n",
    "    if not retrieved_docs:\n",
    "        return \"Error: No relevant recipes found.\"\n",
    "\n",
    "    context = \"\\n\".join([d.page_content for d in retrieved_docs])\n",
    "    prompt = create_prompt(cooking_time)\n",
    "    full_prompt = f\"{context}\\n\\n{prompt}\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            options={\"temperature\": 0.7, \"top_p\": 0.5}  # Adjusted for creative diversity\n",
    "        )\n",
    "        return response[\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: Ollama execution failed. {e}\"\n",
    "\n",
    "# Generate 5 Unique Recipes per Cooking Time\n",
    "times = [2, 5, 10, 20, 30, 40, 50, 60, 90, 120, 150, 180]\n",
    "recipes_dict = {}\n",
    "\n",
    "for idx, time in enumerate(times, start=1):\n",
    "    print(f\"Processing {idx}/{len(times)}: Generating recipes for {time} minutes...\")\n",
    "\n",
    "    unique_recipes = set()\n",
    "    retrieved_docs = retrieve_recipe(f\"Quick meals under {time} minutes\", top_k=5)\n",
    "\n",
    "    while len(unique_recipes) < 5:\n",
    "        recipe = generate_ollama_response(time, retrieved_docs)\n",
    "        if is_unique(recipe, unique_recipes):  # Ensure uniqueness\n",
    "            unique_recipes.add(recipe)\n",
    "\n",
    "    recipes_dict[time] = list(unique_recipes)\n",
    "\n",
    "# Save Recipes to Pickle File\n",
    "with open(\"unique_recipes_old.pkl\", \"wb\") as f:\n",
    "    pickle.dump(recipes_dict, f)\n",
    "\n",
    "print(\"Recipes successfully generated and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open(\"unique_recipes_6.pkl\", \"rb\") as file:\n",
    "    recipes_dict = pickle.load(file)\n",
    "\n",
    "# Display the first 10 rows\n",
    "for time, recipes in list(recipes_dict.items())[:10]:\n",
    "    print(f\"Cooking Time: {time} minutes\")\n",
    "    for recipe in recipes:\n",
    "        print(recipe)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed. Results saved to 'evaluation_results_with_scores.json'.\n",
      "Average Scores: {'uniqueness': 1.0, 'completeness': 0.08333333333333333}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Load the pickle file containing the recipes\n",
    "with open(\"unique_recipes.pkl\", \"rb\") as file:\n",
    "    recipes_dict = pickle.load(file)\n",
    "\n",
    "# Flatten the recipes into a list for evaluation\n",
    "recipes = []\n",
    "for time, recipe_list in recipes_dict.items():\n",
    "    for recipe in recipe_list:\n",
    "        recipes.append({\n",
    "            \"time\": time,\n",
    "            \"recipe\": recipe\n",
    "        })\n",
    "\n",
    "# Parameters for evaluation\n",
    "def evaluate_recipes_with_scores(recipes, threshold=80):\n",
    "    unique_recipes = set()\n",
    "    evaluation_results = []\n",
    "    total_scores = {\"uniqueness\": 0, \"completeness\": 0}\n",
    "    total_recipes = len(recipes)\n",
    "\n",
    "    for entry in recipes:\n",
    "        recipe = entry[\"recipe\"]\n",
    "        time = entry[\"time\"]\n",
    "\n",
    "        # Check for uniqueness\n",
    "        is_unique = all(fuzz.ratio(recipe, r) < threshold for r in unique_recipes)\n",
    "        if is_unique:\n",
    "            unique_recipes.add(recipe)\n",
    "        uniqueness_score = 1 if is_unique else 0\n",
    "\n",
    "        # Check for completeness\n",
    "        has_title = \"Recipe Title:\" in recipe\n",
    "        has_ingredients = \"Ingredients:\" in recipe\n",
    "        has_instructions = \"Instructions:\" in recipe\n",
    "        completeness_score = 1 if has_title and has_ingredients and has_instructions else 0\n",
    "\n",
    "        # Update total scores\n",
    "        total_scores[\"uniqueness\"] += uniqueness_score\n",
    "        total_scores[\"completeness\"] += completeness_score\n",
    "\n",
    "        # Evaluate the recipe\n",
    "        evaluation_results.append({\n",
    "            \"time\": time,\n",
    "            \"recipe\": recipe.strip(),\n",
    "            \"is_unique\": is_unique,\n",
    "            \"has_title\": has_title,\n",
    "            \"has_ingredients\": has_ingredients,\n",
    "            \"has_instructions\": has_instructions,\n",
    "            \"uniqueness_score\": uniqueness_score,\n",
    "            \"completeness_score\": completeness_score,\n",
    "        })\n",
    "\n",
    "    # Calculate average scores\n",
    "    average_scores = {\n",
    "        \"uniqueness\": total_scores[\"uniqueness\"] / total_recipes,\n",
    "        \"completeness\": total_scores[\"completeness\"] / total_recipes,\n",
    "    }\n",
    "\n",
    "    return evaluation_results, average_scores\n",
    "\n",
    "# Evaluate the recipes\n",
    "results, average_scores = evaluate_recipes_with_scores(recipes)\n",
    "\n",
    "# Save the evaluation results to a JSON file\n",
    "with open(\"evaluation_results_with_scores.json\", \"w\") as file:\n",
    "    json.dump({\"results\": results, \"average_scores\": average_scores}, file, indent=4)\n",
    "\n",
    "print(\"Evaluation completed. Results saved to 'evaluation_results_with_scores.json'.\")\n",
    "print(\"Average Scores:\", average_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pickle\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# #  Force CPU Execution\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# # 1. Load LLaMA-2 7B on CPU\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.float32,  \n",
    "#     low_cpu_mem_usage=True\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "# # 3. Recipe Generation\n",
    "# def create_prompt(cooking_time):\n",
    "#     return f\"\"\"<|user|>\n",
    "# You are an expert cook. Generate a recipe that meets the following criteria:\n",
    "\n",
    "# Cooking Time: The total cooking time must be ≤ {cooking_time} minutes. \n",
    "# Cooking includes boiling, baking, heating, frying, sautéing, grilling, roasting, or steaming.\n",
    "# It does not include preparation, refrigeration, or cooling time.\n",
    "\n",
    "# Format: The generated recipe must strictly follow this structure:\n",
    "\n",
    "# Recipe Title: [Clear & concise name]\n",
    "# Ingredients: [List ingredients with exact quantities]\n",
    "# Instructions: [Step-by-step directions]\n",
    "\n",
    "# The recipe must not exceed {cooking_time} minutes under any circumstances. Be precise about the time.\n",
    "# </s>\n",
    "# <|assistant|>\n",
    "# \"\"\"\n",
    "\n",
    "# # 4. Retrieve Relevant Recipes (CPU Only)\n",
    "# def retrieve_recipe(query, top_k=3):\n",
    "#     return ensemble_retriever.get_relevant_documents(query)[:top_k]\n",
    "\n",
    "# # 5. Generate Unique Recipes\n",
    "# def generate_llama_response(cooking_time, retrieved_docs):\n",
    "#     if not retrieved_docs:\n",
    "#         return \"Error: No relevant recipes found.\"\n",
    "\n",
    "#     context = \"\\n\".join([d.page_content for d in retrieved_docs])\n",
    "#     prompt = create_prompt(cooking_time)\n",
    "#     full_prompt = f\"{context}\\n\\n{prompt}\"\n",
    "\n",
    "#     inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "#     # Ensure input tensor is valid\n",
    "#     if inputs.input_ids.shape[1] == 0:\n",
    "#         return \"Error: Model input is empty.\"\n",
    "\n",
    "#     try:\n",
    "#         outputs = model.generate(\n",
    "#             inputs.input_ids,\n",
    "#             max_new_tokens=512,\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.9\n",
    "#         )\n",
    "#         recipe = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#         return recipe.strip()\n",
    "#     except RuntimeError as e:\n",
    "#         return f\"Error: Model execution failed. {e}\"\n",
    "\n",
    "# # 6. Generate 5 Unique Recipes for Each Time and Save as Pickle\n",
    "# times = [2, 5, 10, 20, 30, 40, 50, 60, 90, 120, 150, 180]\n",
    "# recipes_dict = {}\n",
    "\n",
    "# for time in times:\n",
    "#     unique_recipes = set()\n",
    "#     retrieved_docs = retrieve_recipe(f\"Recipes for {time} minutes\", top_k=3)\n",
    "\n",
    "#     while len(unique_recipes) < 5:\n",
    "#         recipe = generate_llama_response(time, retrieved_docs)\n",
    "#         if recipe not in unique_recipes:\n",
    "#             unique_recipes.add(recipe)\n",
    "\n",
    "#     recipes_dict[time] = list(unique_recipes)\n",
    "\n",
    "# # Save to Pickle File\n",
    "# with open(\"unique_recipes.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(recipes_dict, f)\n",
    "\n",
    "# print(\"Recipes successfully generated and saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
